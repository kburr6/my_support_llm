# docker-compose.yml (Final Version)

services:
  # Service 1: The PostgreSQL Database with a robust healthcheck
  db:
    image: ankane/pgvector:latest
    container_name: pgvector-db-compose
    environment:
      - POSTGRES_DB=vector_db
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
    volumes:
      # This volume persists the actual database data across restarts.
      - pg_data:/var/lib/postgresql/data
      # This volume mounts our entire db-init folder into the initialization directory.
      # The PostgreSQL startup script will then find and execute any .sql files inside.
      - ./db-init:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    restart: unless-stopped
    healthcheck:
      # This command checks if the 'ai_configuration' table exists.
      # It will only succeed after our setup.sql script has finished running.
      test: ["CMD-SHELL", "psql -U user -d vector_db -c 'SELECT 1 FROM ai_configuration'"]
      interval: 5s
      timeout: 5s
      retries: 10 # Give the setup script up to 50 seconds to complete

  # Service 2: The Ollama Server
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-compose
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    # The 'deploy' section for GPU acceleration is commented out.
    # Uncomment it if you are deploying on a machine with a compatible NVIDIA GPU
    # and have the NVIDIA Container Toolkit installed.
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Service 3: Your Streamlit Application
  app:
    # 'build: .' tells Docker Compose to build the image from the Dockerfile in this directory.
    build: .
    container_name: support-app-compose
    ports:
      - "8501:8501"
    # This 'depends_on' block is crucial for startup order.
    depends_on:
      db:
        condition: service_healthy # Wait for the db healthcheck to pass before starting.
      ollama:
        condition: service_started # Just wait for the ollama service to have started.
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=ollama # This is used by the app to find the ollama container.
    # This volume is commented out as we are now baking the model into the image.
    # If you revert to the cache-mounting strategy, uncomment this and adjust the path.
    # volumes:
    #   - /mnt/c/Users/YourUser/.cache/huggingface/hub:/root/.cache/huggingface/hub
    
# Service 4: The new Slack Bot Service
  slack-bot:
    # It uses the same build as our Streamlit app because they share the same codebase
    build: .
    container_name: slack-bot-compose
    # Load the secrets from the .env file
    env_file:
      - .env
    # The command to start the FastAPI server
    command: uvicorn slack_bot:app --host 0.0.0.0 --port 8000
    ports:
      - "8000:8000"
    depends_on:
      db:
        condition: service_healthy
      ollama:
        condition: service_started
    restart: unless-stopped

# Define named volumes for persistent data storage.
# This ensures your database and downloaded Ollama models are not lost
# when you stop and restart the containers.
volumes:
  pg_data:
  ollama_data: